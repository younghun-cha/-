{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/younghun-cha/Healthcare-Big-Data-Engineer/blob/main/AI/05-NLP-Transformers/01_NLP_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "school_cell_uuid": "aecf57ef3f9b4c4481e838841b97d1d3",
        "id": "vApjJMI4pGt_"
      },
      "source": [
        "# NLP - Preprocessing 방법론"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "school_cell_uuid": "4e98d1fa82ae4467a4cb19b3374c7537",
        "id": "1SpWtdmppGuc"
      },
      "source": [
        "## 1. DictVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "school_cell_uuid": "24fcf45e4f234364ac0a3ef70d86b22d",
        "id": "bzGkZmMlpGud"
      },
      "source": [
        "- DictVectorizer는 feature_extraction 서브 패키지에서 제공한다.\n",
        "- 문서에서 단어의 사용 빈도를 나타내는 딕셔너리 정보를 입력받아 BOW 인코딩한 수치 벡터로 변환한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "school_cell_uuid": "ec6fd39157174b46bcf16f6fd37bf3e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fnHrqJgpGue",
        "outputId": "bf726722-058b-4465-9162-14d8755f1f64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 2., 0.],\n",
              "       [0., 3., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "v = DictVectorizer(sparse=False)\n",
        "D = [{'A': 1, 'B': 2}, {'B': 3, 'C': 1}]\n",
        "X = v.fit_transform(D)\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "school_cell_uuid": "d017667364754ee1a851df52d36f9a05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbGT7pxvpGuh",
        "outputId": "531ef422-a50b-466d-ce1a-b6985ed4b913"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A', 'B', 'C']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "v.feature_names_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "school_cell_uuid": "aa2df8c3f5fc4da2a7f8087205451069",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTRLocxcpGui",
        "outputId": "e19a6400-d96b-4f38-b9ef-295043b766b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 4.]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "v.transform({'C': 4, 'D': 3})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "school_cell_uuid": "23def73151934f5bbfb54705061dacf7",
        "id": "nYqMUHgRpGuj"
      },
      "source": [
        "## 2. CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "school_cell_uuid": "6a9258a3bfce410e8f0e38392528ec54",
        "id": "DpXtQ_O-pGul"
      },
      "source": [
        "`CountVectorizer`는 다음과 같은 세가지 작업을 수행한다.\n",
        "\n",
        "1. 문서를 토큰 리스트로 변환한다.\n",
        "2. 각 문서에서 토큰의 출현 빈도를 센다.\n",
        "3. 각 문서를 BOW 인코딩 벡터로 변환한다. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "school_cell_uuid": "3e709d78662e4d5aaf4222f7ec748ca8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHbSzs7IpGum",
        "outputId": "d5d9560a-a265-4c6f-a1d6-a6e2f6f83022"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 9,\n",
              " 'is': 3,\n",
              " 'the': 7,\n",
              " 'first': 2,\n",
              " 'document': 1,\n",
              " 'second': 6,\n",
              " 'and': 0,\n",
              " 'third': 8,\n",
              " 'one': 5,\n",
              " 'last': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This is the second second document.',\n",
        "    'And the third one.',\n",
        "    'Is this the first document?',\n",
        "    'The last document?',    \n",
        "]\n",
        "vect = CountVectorizer()\n",
        "vect.fit(corpus)\n",
        "vect.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "school_cell_uuid": "070145846ec14965b9a119d727892539",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2BifYPjpGun",
        "outputId": "f1740605-d53d-4ce6-e1d8-8b4acb634389"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 1, 0, 0, 1, 1, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "vect.transform(['This is the second document.']).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "school_cell_uuid": "33b0491b593c43cca2a79fab3e560f91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnYhKg-npGun",
        "outputId": "858085d3-d780-4880-de9a-b2c1eade904a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "vect.transform(['Something completely new.']).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "school_cell_uuid": "3ec4492b57d04eb7ace34fb644734f42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDqXPgEHpGuo",
        "outputId": "22723647-d103-47a9-ff54-a81bbdefa876"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
              "       [0, 1, 0, 1, 0, 0, 2, 1, 0, 1],\n",
              "       [1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
              "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
              "       [0, 1, 0, 0, 1, 0, 0, 1, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "vect.transform(corpus).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "school_cell_uuid": "cd8d9eb7fb894be3a225a96b7b08a48d",
        "id": "E3vGE42hpGuo"
      },
      "source": [
        "## 3. Stop Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "school_cell_uuid": "674007db62ab4614a1e83af613767fc9",
        "id": "v78SNBHBpGup"
      },
      "source": [
        "- Stop Words 는 문서에서 단어장을 생성할 때 무시할 수 있는 단어를 말한다. \n",
        "- 보통 영어의 관사나 접속사, 한국어의 조사 등이 여기에 해당한다. \n",
        "- `stop_words` 인수로 조절할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "school_cell_uuid": "c19c707f5b7e4f03bd16047c41b5fe2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5GH0aespGuq",
        "outputId": "3f6935dd-4696-4f06-f261-a1178d40a2c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'first': 1, 'document': 0, 'second': 4, 'third': 5, 'one': 3, 'last': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
        "vect.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "school_cell_uuid": "a8f818489aed4d329b1cef167a4a5f2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chLzw0-0pGus",
        "outputId": "c86d3f23-da85-4636-c2a5-d94dd589457c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'document': 0, 'second': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "vect = CountVectorizer(stop_words=\"english\").fit(corpus)\n",
        "vect.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "school_cell_uuid": "e36de7d0c9754b768e50921b97163bb0",
        "id": "YW98R-MLpGut"
      },
      "source": [
        "## 4. 토큰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "school_cell_uuid": "0e5d729a6d8845daa5b7812c2653ac4e",
        "id": "QTPuioibpGut"
      },
      "source": [
        " - `analyzer`, `tokenizer`, `token_pattern` 등의 인수로 사용할 토큰 생성기를 선택할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "school_cell_uuid": "adad581e44824a3d99124d794d8ff6cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgnOsEnNpGuu",
        "outputId": "b9eb9da4-03fd-4ac7-b1de-b45e135a060e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'t': 16,\n",
              " 'h': 8,\n",
              " 'i': 9,\n",
              " 's': 15,\n",
              " ' ': 0,\n",
              " 'e': 6,\n",
              " 'f': 7,\n",
              " 'r': 14,\n",
              " 'd': 5,\n",
              " 'o': 13,\n",
              " 'c': 4,\n",
              " 'u': 17,\n",
              " 'm': 11,\n",
              " 'n': 12,\n",
              " '.': 1,\n",
              " 'a': 3,\n",
              " '?': 2,\n",
              " 'l': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "vect = CountVectorizer(analyzer=\"char\").fit(corpus)\n",
        "vect.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "school_cell_uuid": "fb8a5de3db2a4fa5a5bcbe0c481f2c1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UnH1ZtEpGuv",
        "outputId": "03d30257-2c45-4b2b-e7dd-97ee89c20ce9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 2, 'the': 0, 'third': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "vect = CountVectorizer(token_pattern=\"t\\w+\").fit(corpus)\n",
        "vect.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "school_cell_uuid": "67bb95b158cc4e8ea86b98de62aa9276",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEWfYldZpGuw",
        "outputId": "544273d5-cce6-4367-a5ea-091c09fc68f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 11,\n",
              " 'is': 5,\n",
              " 'the': 9,\n",
              " 'first': 4,\n",
              " 'document': 3,\n",
              " '.': 0,\n",
              " 'second': 8,\n",
              " 'and': 2,\n",
              " 'third': 10,\n",
              " 'one': 7,\n",
              " '?': 1,\n",
              " 'last': 6}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "vect = CountVectorizer(tokenizer=nltk.word_tokenize).fit(corpus)\n",
        "vect.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "school_cell_uuid": "792d8a8be7c74d3199ee6f396e4dca8d",
        "id": "-mDdJDPVpGuw"
      },
      "source": [
        "## 5. n-그램"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "school_cell_uuid": "ce471098be3944c99b774b92427aa8b6",
        "id": "pvfRQaY8pGux"
      },
      "source": [
        "- n-그램은 단어장 생성에 사용할 토큰의 크기를 결정한다. \n",
        "- 모노그램(1-그램)은 토큰 하나만 단어로 사용하며 바이그램(2-그램)은 두 개의 연결된 토큰을 하나의 단어로 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "school_cell_uuid": "16377ccb9f3f4eb48a75ec3f578ba006",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KODnJfO6pGux",
        "outputId": "9a83277f-b86b-42e1-c020-0262ce404e7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this is': 12,\n",
              " 'is the': 2,\n",
              " 'the first': 7,\n",
              " 'first document': 1,\n",
              " 'the second': 9,\n",
              " 'second second': 6,\n",
              " 'second document': 5,\n",
              " 'and the': 0,\n",
              " 'the third': 10,\n",
              " 'third one': 11,\n",
              " 'is this': 3,\n",
              " 'this the': 13,\n",
              " 'the last': 8,\n",
              " 'last document': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "vect = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
        "vect.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "school_cell_uuid": "cfb6d9eb76cf447da36031a6d30b266e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9K0gUEnpGux",
        "outputId": "59243d28-05fd-4d86-8903-2e1009db5d83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 3, 'the': 0, 'this the': 4, 'third': 2, 'the third': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "vect = CountVectorizer(ngram_range=(1, 2), token_pattern=\"t\\w+\").fit(corpus)\n",
        "vect.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "school_cell_uuid": "26692f9b92884fa992ac62776fecb56e",
        "id": "W_4JN6SJpGuy"
      },
      "source": [
        "## 6. 빈도수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "school_cell_uuid": "0bf059b319ec4f91b24cc57b14258524",
        "id": "AAn1I13RpGuy"
      },
      "source": [
        "- `max_df`, `min_df` 인수를 사용하여 문서에서 토큰이 나타난 횟수를 기준으로 단어장을 구성할 수도 있다. \n",
        "- 토큰의 빈도가 `max_df`로 지정한 값을 초과 하거나 `min_df`로 지정한 값보다 작은 경우에는 무시한다. \n",
        "- 인수 값은 정수인 경우 횟수, 부동소수점인 경우 비중을 뜻한다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "school_cell_uuid": "44a67b98b54d4eb2a85c1b513bca6fef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJUwm04HpGuz",
        "outputId": "475d55c7-e380-4d04-f1d0-43dbaf2116f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'this': 3, 'is': 2, 'first': 1, 'document': 0},\n",
              " {'and', 'last', 'one', 'second', 'the', 'third'})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "vect = CountVectorizer(max_df=4, min_df=2).fit(corpus)\n",
        "vect.vocabulary_, vect.stop_words_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "school_cell_uuid": "e8058587c97949c8a95362af3c12893c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eznR8u4NpGu0",
        "outputId": "3b52bed1-9883-449c-f9c3-dccc41847f1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 2, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "vect.transform(corpus).toarray().sum(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "school_cell_uuid": "65e8556f8bce4860be040ae0fffb3768",
        "id": "LBmd7Rp5pGu2"
      },
      "source": [
        "## 7. TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "school_cell_uuid": "e937920a0d87431581826d6cf30ea2a1",
        "id": "OTUulJnppGu4"
      },
      "source": [
        "- TF-IDF(Term Frequency – Inverse Document Frequency) 인코딩은 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소하는 방법이다. \n",
        "\n",
        "\n",
        "- 구제적으로는 문서 $d$(document)와 단어 $t$ 에 대해 다음과 같이 계산한다.\n",
        "\n",
        "$$ \\text{tf-idf}(d, t) = \\text{tf}(d, t) \\cdot \\text{idf}(t) $$\n",
        "\n",
        "\n",
        "여기에서\n",
        "\n",
        "* $\\text{tf}(d, t)$: term frequency. 특정한 단어의 빈도수\n",
        "* $\\text{idf}(t)$ : inverse document frequency. 특정한 단어가 들어 있는 문서의 수에 반비례하는 수\n",
        " \n",
        " $$ \\text{idf}(d, t) = \\log \\dfrac{n}{1 + \\text{df}(t)} $$\n",
        " \n",
        "* $n$ : 전체 문서의 수\n",
        "* $\\text{df}(t)$:  단어 $t$를 가진 문서의 수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "school_cell_uuid": "3292542e3194487e9208ca1fb3671d4e",
        "id": "knmTjQHDpGu5"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "school_cell_uuid": "c4bb72733a504c558e5c61c245d62a63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kijr8r9opGu6",
        "outputId": "516e78e9-b713-4ad9-fb34-563a4e8de42d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.38947624, 0.55775063, 0.4629834 , 0.        ,\n",
              "        0.        , 0.        , 0.32941651, 0.        , 0.4629834 ],\n",
              "       [0.        , 0.24151532, 0.        , 0.28709733, 0.        ,\n",
              "        0.        , 0.85737594, 0.20427211, 0.        , 0.28709733],\n",
              "       [0.55666851, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.55666851, 0.        , 0.26525553, 0.55666851, 0.        ],\n",
              "       [0.        , 0.38947624, 0.55775063, 0.4629834 , 0.        ,\n",
              "        0.        , 0.        , 0.32941651, 0.        , 0.4629834 ],\n",
              "       [0.        , 0.45333103, 0.        , 0.        , 0.80465933,\n",
              "        0.        , 0.        , 0.38342448, 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "tfidv = TfidfVectorizer().fit(corpus)\n",
        "tfidv.transform(corpus).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "school_cell_uuid": "071c8034b0ad4ba49879832c487592ec",
        "id": "TQtGn1RVpGu8"
      },
      "source": [
        "## 8.Hashing Trick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "school_cell_uuid": "ae25f3c0e0dc480496dfe0cba85b23bb",
        "id": "toq97BzWpGu-"
      },
      "source": [
        "- `CountVectorizer`는 모든 작업을 메모리 상에서 수행하므로 처리할 문서의 크기가 커지면 속도가 느려지거나 실행이 불가능해진다. \n",
        "- 이 때  `HashingVectorizer`를 사용하면 해시 함수를 사용하여 단어에 대한 인덱스 번호를 생성하기 때문에 메모리 및 실행 시간을 줄일 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "school_cell_uuid": "44975509dd7d41bc9a0163faa36ff4ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aIRaJE6pGu_",
        "outputId": "8e702426-f505-4773-cd12-72faf45b9857"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "twenty = fetch_20newsgroups()\n",
        "len(twenty.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "school_cell_uuid": "81e28a59c9ca4747959ae4bba434b95d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkzV-S5ZpGvA",
        "outputId": "da7bab78-75a8-4626-ac50-554bb297c742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 10.2 s, sys: 111 ms, total: 10.3 s\n",
            "Wall time: 14.9 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<11314x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 1787565 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "%time CountVectorizer().fit(twenty.data).transform(twenty.data);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "school_cell_uuid": "baf82c040ffd4710a455eabfb2686251",
        "id": "XaE2G2FnpGvA"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "hv = HashingVectorizer(n_features=300000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "school_cell_uuid": "f14946419c1f482bbad18c1afe78cec8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRhHE39KpGvA",
        "outputId": "6c0b8b70-53c3-4018-db67-b60176b39a29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.37 s, sys: 41.2 ms, total: 3.41 s\n",
            "Wall time: 5.78 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<11314x300000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 1786336 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "%time hv.transform(twenty.data);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpMUaao8pGvB"
      },
      "source": [
        "## 9. BPE(Bi Pair Encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTg_kPpypGvB",
        "outputId": "e917d2bb-8b09-4090-957a-ca6a226d621b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "('e', 's')\n",
            "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n",
            "\n",
            "\n",
            "Step 2\n",
            "('es', 't')\n",
            "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n",
            "\n",
            "\n",
            "Step 3\n",
            "('est', '</w>')\n",
            "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 4\n",
            "('l', 'o')\n",
            "{'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 5\n",
            "('lo', 'w')\n",
            "{'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 6\n",
            "('n', 'e')\n",
            "{'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 7\n",
            "('ne', 'w')\n",
            "{'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 8\n",
            "('new', 'est</w>')\n",
            "{'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 9\n",
            "('low', '</w>')\n",
            "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 10\n",
            "('w', 'i')\n",
            "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re, collections\n",
        "\n",
        "def get_stats(vocab):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,\n",
        "'n e w e s t </w>':6, 'w i d e s t </w>':3}\n",
        "num_merges = 10\n",
        "\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(vocab)\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "    print(f'Step {i + 1}')\n",
        "    print(best)\n",
        "    print(vocab)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HTtt3BypGvB",
        "outputId": "2f35d2b3-f585-4cd3-fa68-6bed4ef907e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'나 는': 1,\n",
              " '책 상': 1,\n",
              " '위 에': 1,\n",
              " '사 과 를': 2,\n",
              " '먹 었 다': 1,\n",
              " '알 고': 1,\n",
              " '보 니': 1,\n",
              " '그': 1,\n",
              " '사 과 는': 1,\n",
              " 'J a s o n': 1,\n",
              " '것 이 었 다': 1,\n",
              " '그 래 서': 1,\n",
              " 'J a s o n 에 게': 1,\n",
              " '했 다': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "s1 = '나는 책상 위에 사과를 먹었다'       \n",
        "s2 = '알고 보니 그 사과는 Jason 것이었다' \n",
        "s3 = '그래서 Jason에게 사과를 했다'\n",
        "\n",
        "token_counts = {}\n",
        "index = 0\n",
        "\n",
        "for sentence in [s1, s2, s3]:\n",
        "    tokens = sentence.split()\n",
        "    for token in tokens:\n",
        "        if token_counts.get(token) == None:\n",
        "            token_counts[token] = 1\n",
        "        else:\n",
        "            token_counts[token] += 1\n",
        "\n",
        "token_counts = {\" \".join(token) : counts for token, counts in token_counts.items()}\n",
        "token_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BO9knFgQpGvC",
        "outputId": "6addaf6c-bf79-4ff2-809f-d331616e1c24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "('사', '과')\n",
            "{'나 는': 1, '책 상': 1, '위 에': 1, '사과 를': 2, '먹 었 다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'J a s o n': 1, '것 이 었 다': 1, '그 래 서': 1, 'J a s o n 에 게': 1, '했 다': 1}\n",
            "\n",
            "\n",
            "Step 2\n",
            "('사과', '를')\n",
            "{'나 는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었 다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'J a s o n': 1, '것 이 었 다': 1, '그 래 서': 1, 'J a s o n 에 게': 1, '했 다': 1}\n",
            "\n",
            "\n",
            "Step 3\n",
            "('었', '다')\n",
            "{'나 는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'J a s o n': 1, '것 이 었다': 1, '그 래 서': 1, 'J a s o n 에 게': 1, '했 다': 1}\n",
            "\n",
            "\n",
            "Step 4\n",
            "('J', 'a')\n",
            "{'나 는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Ja s o n': 1, '것 이 었다': 1, '그 래 서': 1, 'Ja s o n 에 게': 1, '했 다': 1}\n",
            "\n",
            "\n",
            "Step 5\n",
            "('Ja', 's')\n",
            "{'나 는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Jas o n': 1, '것 이 었다': 1, '그 래 서': 1, 'Jas o n 에 게': 1, '했 다': 1}\n",
            "\n",
            "\n",
            "Step 6\n",
            "('Jas', 'o')\n",
            "{'나 는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Jaso n': 1, '것 이 었다': 1, '그 래 서': 1, 'Jaso n 에 게': 1, '했 다': 1}\n",
            "\n",
            "\n",
            "Step 7\n",
            "('Jaso', 'n')\n",
            "{'나 는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Jason': 1, '것 이 었다': 1, '그 래 서': 1, 'Jason 에 게': 1, '했 다': 1}\n",
            "\n",
            "\n",
            "Step 8\n",
            "('나', '는')\n",
            "{'나는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Jason': 1, '것 이 었다': 1, '그 래 서': 1, 'Jason 에 게': 1, '했 다': 1}\n",
            "\n",
            "\n",
            "Step 9\n",
            "('책', '상')\n",
            "{'나는': 1, '책상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Jason': 1, '것 이 었다': 1, '그 래 서': 1, 'Jason 에 게': 1, '했 다': 1}\n",
            "\n",
            "\n",
            "Step 10\n",
            "('위', '에')\n",
            "{'나는': 1, '책상': 1, '위에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Jason': 1, '것 이 었다': 1, '그 래 서': 1, 'Jason 에 게': 1, '했 다': 1}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "num_merges = 10\n",
        "\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(token_counts)\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    token_counts = merge_vocab(best, token_counts)\n",
        "    print(f'Step {i + 1}')\n",
        "    print(best)\n",
        "    print(token_counts)\n",
        "    print('\\n')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}